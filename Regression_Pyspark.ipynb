{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6eb9f6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/phuhien/PycharmProjects/ocvx/venv/lib/python3.10/site-packages (3.2.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/phuhien/PycharmProjects/ocvx/venv/lib/python3.10/site-packages (from pyspark) (0.10.9.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1.2 is available.\r\n",
      "You should consider upgrading via the '/home/phuhien/PycharmProjects/ocvx/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89cac50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d971bc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Application_Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "446ad8ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/apache-spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/15 02:23:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/15 02:23:55 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m (\u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark_application_name\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/pyspark/sql/session.py:228\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    226\u001B[0m         sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[0;32m--> 228\u001B[0m     sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[1;32m    231\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc)\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/pyspark/context.py:392\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 392\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/pyspark/context.py:146\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    144\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaster\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparkHome\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpyFiles\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menvironment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatchSize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserializer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m                  \u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjsc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofiler_cls\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[39;00m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/pyspark/context.py:209\u001B[0m, in \u001B[0;36mSparkContext._do_init\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menvironment[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPYTHONHASHSEED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    208\u001B[0m \u001B[38;5;66;03m# Create the Java SparkContext through Py4J\u001B[39;00m\n\u001B[0;32m--> 209\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc \u001B[38;5;241m=\u001B[39m jsc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[39;00m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;241m=\u001B[39m SparkConf(_jconf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39mconf())\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/pyspark/context.py:329\u001B[0m, in \u001B[0;36mSparkContext._initialize_context\u001B[0;34m(self, jconf)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_initialize_context\u001B[39m(\u001B[38;5;28mself\u001B[39m, jconf):\n\u001B[1;32m    326\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001B[39;00m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 329\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mJavaSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjconf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1585\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1579\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1580\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1581\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1582\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1584\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1585\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1586\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1588\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1589\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/PycharmProjects/ocvx/venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34643dc6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m filePath \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msf-stocks-clean.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m stocksDF \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(filePath)\n\u001B[1;32m      3\u001B[0m display(stocksDF)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "filePath = \"sf-stocks-clean.parquet\"\n",
    "stocksDF = spark.read.parquet(filePath)\n",
    "display(stocksDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542dd9d5-253a-4dba-8d0c-ac924696d3b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56428b38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5102 rows in the training set, and 1231 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = stocksDF.randomSplit([.8, .2], seed=42)\n",
    "print(f\"There are {trainDF.cache().count()} rows in the training set, and {testDF.cache().count()} in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d32faf6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|              High|             Volume|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|              5102|               5102|\n",
      "|   mean| 536.2721293407158|3.637840187612701E7|\n",
      "| stddev| 699.8217961274913|5.058271778480273E7|\n",
      "|    min|29.127500534057617|           347500.0|\n",
      "|    25%| 70.80000305175781|          3737600.0|\n",
      "|    50%| 164.6300048828125|          1.88807E7|\n",
      "|    75%|             995.5|           4.0798E7|\n",
      "|    max|           3552.25|           4.4794E8|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.select(\"Low\",\"Volume\").summary().show()\n",
    "# DK WHAT TO DO HERE ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ebe33-193c-4e94-b285-ba095a639f8c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8f46e3-f22d-498b-985f-e891c820b8ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+\n",
      "|   Volume|   features|              High|\n",
      "+---------+-----------+------------------+\n",
      "|2.96165E7|[2.96165E7]|44.066001892089844|\n",
      "|5.60675E7|[5.60675E7]|45.599998474121094|\n",
      "|2.76395E7|[2.76395E7]|  46.0620002746582|\n",
      "|1.98975E7|[1.98975E7]| 46.38399887084961|\n",
      "|   1.83E7|   [1.83E7]|46.400001525878906|\n",
      "| 1.8951E7| [1.8951E7]| 46.13999938964844|\n",
      "|2.30875E7|[2.30875E7]|47.992000579833984|\n",
      "| 1.8845E7| [1.8845E7]| 47.94200134277344|\n",
      "|3.86615E7|[3.86615E7]|49.736000061035156|\n",
      "|2.10215E7|[2.10215E7]| 49.20000076293945|\n",
      "+---------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"Volume\"], outputCol=\"features\")\n",
    "\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "\n",
    "vecTrainDF.select(\"Volume\", \"features\", \"Low\").show(10)\n",
    "\n",
    "# We combine the column Volume and Low into 1 single vector column called \"features\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edd7f7-b84b-432e-ae6c-288eb7c9e5d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa41d93e-b9a2-48d8-ac5c-852ca08e5a8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Low\", regParam=0.01)\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "244b3267-675f-465b-b9f6-d972b5e9cfde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is High = -0.0*Volume + 743.05\n"
     ]
    }
   ],
   "source": [
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "\n",
    "print(f\"The formula for the linear regression line is Low = {m}*Volume + {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7223d-2e5d-40d7-a73f-1004132b1585",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce0be07-6b77-4fb0-b1cc-1965af422e79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d8bb0-4f97-46dc-8eed-c6b24743838c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply to Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61e673ba-3850-410a-8d96-1199f13abad9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------+-----------------+\n",
      "|   Volume|   features|              High|       prediction|\n",
      "+---------+-----------+------------------+-----------------+\n",
      "|2.95585E7|[2.95585E7]| 45.49599838256836|575.0362520479239|\n",
      "| 1.8254E7| [1.8254E7]| 45.99599838256836|639.2906992676967|\n",
      "| 3.0465E7| [3.0465E7]| 47.56999969482422|569.8837325729604|\n",
      "|3.13145E7|[3.13145E7]|50.178001403808594|565.0551994301238|\n",
      "|2.05805E7|[2.05805E7]|51.178001403808594|626.0669413432594|\n",
      "|1.78125E7|[1.78125E7]| 51.56399917602539|641.8001723489944|\n",
      "| 3.6726E7| [3.6726E7]| 57.47800064086914|534.2963917756675|\n",
      "| 7.4576E7| [7.4576E7]| 52.93199920654297|319.1580993020153|\n",
      "|1.93965E7|[1.93965E7]|49.731998443603516|632.7967626983136|\n",
      "|1.53315E7|[1.53315E7]| 49.29999923706055|655.9021037526174|\n",
      "+---------+-----------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "predDF.select(\"Volume\", \"features\", \"Low\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec045a-f7bd-4035-b8b3-1d22fc5f4712",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbb6706f-b2e3-4305-a4f3-ac2f11d20e8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE for predicting the High is: 651.36\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionMeanEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Low\", metricName=\"rmse\")\n",
    "\n",
    "print(f\"The RMSE for predicting the Low is: {regressionMeanEvaluator.evaluate(predDF):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}