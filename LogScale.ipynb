{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e69a31fe-1351-402c-81c1-d11425123634",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e69a31fe-1351-402c-81c1-d11425123634"
      },
      "source": [
        "# Log Scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8ca4e472-f7b1-4a64-80a3-83bd0831c25a",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ca4e472-f7b1-4a64-80a3-83bd0831c25a",
        "outputId": "0c172e9c-a6ac-4eaa-a19e-2ee5039e53ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 6.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=47a31ad081b8fd69b001bc09c12064c2cc0406c106234b923d62bfd37c2af603\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ee1a105-f986-454c-b614-1e5c1f7024cb",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5ee1a105-f986-454c-b614-1e5c1f7024cb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8790f225-d2fc-4b00-95f5-2d6dc75ee8ca",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8790f225-d2fc-4b00-95f5-2d6dc75ee8ca"
      },
      "outputs": [],
      "source": [
        "spark_application_name = \"Spark_Application_Name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ceb4c9bb-2055-43e5-a7a0-f6c088b33216",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ceb4c9bb-2055-43e5-a7a0-f6c088b33216"
      },
      "outputs": [],
      "source": [
        "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "917f4372-c876-40eb-bbc7-a1e02c93e32c",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "917f4372-c876-40eb-bbc7-a1e02c93e32c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "from pyspark.sql import Window\n",
        "\n",
        "filePath = \"stocks-final.parquet\"\n",
        "stocksDF = spark.read.parquet(filePath)\n",
        "\n",
        "stocksDF = stocksDF.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"Date\"))).drop(\"company_name\")\n",
        "trainDF = stocksDF.where(\"rank <= .8\").drop(\"rank\")\n",
        "testDF = stocksDF.where(\"rank > .8\").drop(\"rank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c280d0e3-5edf-4af3-a13f-5f0ead90ef95",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "c280d0e3-5edf-4af3-a13f-5f0ead90ef95"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, log\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "logTrainDF = trainDF.withColumn(\"log_next\", log(col(\"Next\")))\n",
        "logTestDF = testDF.withColumn(\"log_next\", log(col(\"Next\")))\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "numericCols = []\n",
        "for (field, dataType) in trainDF.dtypes:\n",
        "    if (dataType == \"double\") & (field != \"Next\"):\n",
        "        numericCols.append(field)\n",
        "\n",
        "vecAssembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
        "\n",
        "lr = LinearRegression(labelCol=\"log_next\", predictionCol=\"log_pred\")\n",
        "pipeline = Pipeline(stages = [vecAssembler, lr])\n",
        "pipelineModel = pipeline.fit(logTrainDF)\n",
        "predictionDF = pipelineModel.transform(logTestDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b527a096-14f9-4bb9-b203-61ff8bc813cc",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "b527a096-14f9-4bb9-b203-61ff8bc813cc"
      },
      "source": [
        "## Exponentiate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5620c44c-6877-44c4-961b-3f990a135e47",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5620c44c-6877-44c4-961b-3f990a135e47",
        "outputId": "655df1b6-62d6-4cc5-e1bf-5ac420088d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE is 107.45571599482726\n",
            "R2 is 0.6096864492272256\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, exp\n",
        "\n",
        "expDF = predictionDF.withColumn(\"prediction\", exp(col(\"log_pred\")))\n",
        "\n",
        "regEvaluator = RegressionEvaluator(labelCol=\"Next\", predictionCol=\"prediction\")\n",
        "rmse = regEvaluator.setMetricName(\"rmse\").evaluate(expDF)\n",
        "r2 = regEvaluator.setMetricName(\"r2\").evaluate(expDF)\n",
        "print(f\"RMSE is {rmse}\")\n",
        "print(f\"R2 is {r2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80d13487-4f84-457c-ac7c-09db95943049",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "80d13487-4f84-457c-ac7c-09db95943049"
      },
      "source": [
        "## Apply to Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5f6ab85f-15e9-4c9b-a719-0cdbdc5c1561",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6ab85f-15e9-4c9b-a719-0cdbdc5c1561",
        "outputId": "23e7b8b4-b91f-4220-ca48-04007e733496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+------------------+\n",
            "|            features|              Next|        prediction|\n",
            "+--------------------+------------------+------------------+\n",
            "|[1436.96997070312...|1438.1400146484375|1477.1218308939633|\n",
            "|[1438.14001464843...| 1415.699951171875|1460.0566315762965|\n",
            "|[1415.69995117187...|1371.7039794921875|1450.0198583333124|\n",
            "|[1371.70397949218...|1341.1400146484375|1371.7448274763638|\n",
            "|[1341.14001464843...|1390.8699951171875|1399.9846185911151|\n",
            "|[1390.86999511718...|1410.1500244140625|1455.2577405579018|\n",
            "|[1410.15002441406...|1388.0899658203125|1418.3932186939137|\n",
            "|[1388.08996582031...|1358.9100341796875|1439.6753633039857|\n",
            "|[1358.91003417968...| 1306.219970703125| 1367.325193292934|\n",
            "|[1306.21997070312...| 1254.760009765625|  1331.44913530293|\n",
            "+--------------------+------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "expDF.select(\"features\", \"Next\",\"prediction\").show(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "name": "4LogScale.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}