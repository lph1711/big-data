{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fe15e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/phuhien/anaconda3/envs/ocvx/lib/python3.7/site-packages (2.4.0)\r\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/phuhien/anaconda3/envs/ocvx/lib/python3.7/site-packages (from pyspark) (0.10.7)\r\n"
     ]
    }
   ],
   "source": [
    " !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037b6898",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5535eb87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Application_Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c0a0f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/apache-spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/16 16:44:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "22/06/16 16:44:19 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_4241/2149014912.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mspark\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mSparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspark_application_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    171\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m                         \u001B[0msparkConf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m                     \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msparkConf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    174\u001B[0m                     \u001B[0;31m# This SparkContext may be an existing one.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m                     \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_options\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36mgetOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    347\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    348\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 349\u001B[0;31m                 \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    350\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0;32m--> 118\u001B[0;31m                           conf, jsc, profiler_cls)\n\u001B[0m\u001B[1;32m    119\u001B[0m         \u001B[0;32mexcept\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_do_init\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    178\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m         \u001B[0;31m# Create the Java SparkContext through Py4J\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 180\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjsc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_initialize_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    181\u001B[0m         \u001B[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkConf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_jconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_initialize_context\u001B[0;34m(self, jconf)\u001B[0m\n\u001B[1;32m    286\u001B[0m         \u001B[0mInitialize\u001B[0m \u001B[0mSparkContext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0mto\u001B[0m \u001B[0mallow\u001B[0m \u001B[0msubclass\u001B[0m \u001B[0mspecific\u001B[0m \u001B[0minitialization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    287\u001B[0m         \"\"\"\n\u001B[0;32m--> 288\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mJavaSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    289\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    290\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mclassmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1523\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1524\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1525\u001B[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001B[0m\u001B[1;32m   1526\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1527\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/ocvx/lib/python3.7/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:459)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:448)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119fd1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m tesla_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstocks_data/TESLA.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m zoom_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstocks_data/ZOOM.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 9\u001B[0m rawDF_amazon \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(amazon_path, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, multiLine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, escape\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m rawDF_apple \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(apple_path, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, multiLine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, escape\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     11\u001B[0m rawDF_facebook \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mcsv(facebook_path, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, inferSchema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, multiLine\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m, escape\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "amazon_csv = \"stocks_data/AMAZON.csv\"\n",
    "apple_csv = \"stocks_data/APPLE.csv\"\n",
    "facebook_csv = \"stocks_data/FACEBOOK.csv\"\n",
    "google_csv = \"stocks_data/GOOGLE.csv\"\n",
    "microsoft_csv = \"stocks_data/MICROSOFT.csv\"\n",
    "tesla_csv = \"stocks_data/TESLA.csv\"\n",
    "zoom_csv = \"stocks_data/ZOOM.csv\"\n",
    "\n",
    "unclean_df_amazon = spark.read.csv(amazon_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "unclean_df_apple = spark.read.csv(apple_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "unclean_df_facebook = spark.read.csv(facebook_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "unclean_df_google = spark.read.csv(google_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "unclean_df_microsoft = spark.read.csv(microsoft_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "unclean_df_tesla = spark.read.csv(tesla_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "unclean_df_zoom = spark.read.csv(zoom_csv, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3babfa48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "dfs = [unclean_df_amazon, unclean_df_apple, unclean_df_facebook, unclean_df_google, unclean_df_microsoft, unclean_df_tesla, unclean_df_zoom]\n",
    "df = reduce(DataFrame.union, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c84008de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 7) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|company_name|\n",
      "+------------+\n",
      "|      AMAZON|\n",
      "|       APPLE|\n",
      "|    FACEBOOK|\n",
      "|      GOOGLE|\n",
      "|   MICROSOFT|\n",
      "|       TESLA|\n",
      "|        ZOOM|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"company_name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56a467c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, High: double, Low: double, Open: double, Close: double, Volume: double, Adj Close: double, company_name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf67a292",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd58f15c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25999252",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+------------+\n",
      "|      Date|             High|              Low|             Open|            Close|   Volume|        Adj Close|company_name|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+------------+\n",
      "|2017-01-03| 758.760009765625|747.7000122070312|757.9199829101562|753.6699829101562|3521100.0|753.6699829101562|      AMAZON|\n",
      "|2017-01-04|759.6799926757812|754.2000122070312|758.3900146484375|757.1799926757812|2510500.0|757.1799926757812|      AMAZON|\n",
      "|2017-01-05|782.4000244140625| 760.260009765625|761.5499877929688|780.4500122070312|5830100.0|780.4500122070312|      AMAZON|\n",
      "|2017-01-06|799.4400024414062|  778.47998046875|782.3599853515625| 795.989990234375|5986200.0| 795.989990234375|      AMAZON|\n",
      "|2017-01-09|  801.77001953125|  791.77001953125|            798.0|796.9199829101562|3446100.0|796.9199829101562|      AMAZON|\n",
      "|2017-01-10|            798.0|789.5399780273438|796.5999755859375|795.9000244140625|2558400.0|795.9000244140625|      AMAZON|\n",
      "|2017-01-11|            799.5| 789.510009765625|793.6599731445312|  799.02001953125|2992800.0|  799.02001953125|      AMAZON|\n",
      "|2017-01-12|814.1300048828125|            799.5|800.3099975585938|813.6400146484375|4873900.0|813.6400146484375|      AMAZON|\n",
      "|2017-01-13|821.6500244140625|811.4000244140625|814.3200073242188|817.1400146484375|3791900.0|817.1400146484375|      AMAZON|\n",
      "|2017-01-17|            816.0|803.4400024414062|815.7000122070312| 809.719970703125|3670500.0| 809.719970703125|      AMAZON|\n",
      "|2017-01-18|  811.72998046875|  804.27001953125|            809.5|  807.47998046875|2354200.0|  807.47998046875|      AMAZON|\n",
      "|2017-01-19| 813.510009765625|807.3200073242188|            810.0|809.0399780273438|2540800.0|809.0399780273438|      AMAZON|\n",
      "|2017-01-20|  816.02001953125| 806.260009765625| 815.280029296875|808.3300170898438|3376200.0|808.3300170898438|      AMAZON|\n",
      "|2017-01-23|            818.5|805.0800170898438|806.7999877929688|817.8800048828125|2797500.0|817.8800048828125|      AMAZON|\n",
      "|2017-01-24| 823.989990234375|            814.5|            822.0|822.4400024414062|2971700.0|822.4400024414062|      AMAZON|\n",
      "|2017-01-25|837.4199829101562|825.2899780273438|825.7899780273438|  836.52001953125|3922600.0|  836.52001953125|      AMAZON|\n",
      "|2017-01-26|843.8400268554688|            833.0| 835.530029296875|839.1500244140625|3586300.0|839.1500244140625|      AMAZON|\n",
      "|2017-01-27|839.7000122070312|829.4400024414062|            839.0|  835.77001953125|2998700.0|  835.77001953125|      AMAZON|\n",
      "|2017-01-30|            833.5|816.3800048828125|            833.0|830.3800048828125|3747300.0|830.3800048828125|      AMAZON|\n",
      "|2017-01-31| 826.989990234375|819.5599975585938|           823.75|  823.47998046875|3137200.0|  823.47998046875|      AMAZON|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a6f78e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d842c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68c06be7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+------------+\n",
      "|      Date|             High|              Low|             Open|            Close|   Volume|        Adj Close|company_name|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+------------+\n",
      "|2017-01-03| 758.760009765625|747.7000122070312|757.9199829101562|753.6699829101562|3521100.0|753.6699829101562|      AMAZON|\n",
      "|2017-01-04|759.6799926757812|754.2000122070312|758.3900146484375|757.1799926757812|2510500.0|757.1799926757812|      AMAZON|\n",
      "|2017-01-05|782.4000244140625| 760.260009765625|761.5499877929688|780.4500122070312|5830100.0|780.4500122070312|      AMAZON|\n",
      "|2017-01-06|799.4400024414062|  778.47998046875|782.3599853515625| 795.989990234375|5986200.0| 795.989990234375|      AMAZON|\n",
      "|2017-01-09|  801.77001953125|  791.77001953125|            798.0|796.9199829101562|3446100.0|796.9199829101562|      AMAZON|\n",
      "|2017-01-10|            798.0|789.5399780273438|796.5999755859375|795.9000244140625|2558400.0|795.9000244140625|      AMAZON|\n",
      "|2017-01-11|            799.5| 789.510009765625|793.6599731445312|  799.02001953125|2992800.0|  799.02001953125|      AMAZON|\n",
      "|2017-01-12|814.1300048828125|            799.5|800.3099975585938|813.6400146484375|4873900.0|813.6400146484375|      AMAZON|\n",
      "|2017-01-13|821.6500244140625|811.4000244140625|814.3200073242188|817.1400146484375|3791900.0|817.1400146484375|      AMAZON|\n",
      "|2017-01-17|            816.0|803.4400024414062|815.7000122070312| 809.719970703125|3670500.0| 809.719970703125|      AMAZON|\n",
      "|2017-01-18|  811.72998046875|  804.27001953125|            809.5|  807.47998046875|2354200.0|  807.47998046875|      AMAZON|\n",
      "|2017-01-19| 813.510009765625|807.3200073242188|            810.0|809.0399780273438|2540800.0|809.0399780273438|      AMAZON|\n",
      "|2017-01-20|  816.02001953125| 806.260009765625| 815.280029296875|808.3300170898438|3376200.0|808.3300170898438|      AMAZON|\n",
      "|2017-01-23|            818.5|805.0800170898438|806.7999877929688|817.8800048828125|2797500.0|817.8800048828125|      AMAZON|\n",
      "|2017-01-24| 823.989990234375|            814.5|            822.0|822.4400024414062|2971700.0|822.4400024414062|      AMAZON|\n",
      "|2017-01-25|837.4199829101562|825.2899780273438|825.7899780273438|  836.52001953125|3922600.0|  836.52001953125|      AMAZON|\n",
      "|2017-01-26|843.8400268554688|            833.0| 835.530029296875|839.1500244140625|3586300.0|839.1500244140625|      AMAZON|\n",
      "|2017-01-27|839.7000122070312|829.4400024414062|            839.0|  835.77001953125|2998700.0|  835.77001953125|      AMAZON|\n",
      "|2017-01-30|            833.5|816.3800048828125|            833.0|830.3800048828125|3747300.0|830.3800048828125|      AMAZON|\n",
      "|2017-01-31| 826.989990234375|819.5599975585938|           823.75|  823.47998046875|3137200.0|  823.47998046875|      AMAZON|\n",
      "+----------+-----------------+-----------------+-----------------+-----------------+---------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noNullsDF = df.na.fill(value=0, subset=[\"Volume\"])\n",
    "noNullsDF.show()\n",
    "\n",
    "# We can see that there are 987 Volume columns in apple, microsoft and tesla DFs have null values.\n",
    "# We do not want to remove every rows that contain a null value because if we do so 987 rows in 3 DFs will be removed,\n",
    "# which also means a lot of data from other columns of that row will be lost, and these data is important in the analytics of other information of that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03255495",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Don't need to impute cast to doubles cause already done\n",
    "# Some of airbnb data was BS so had to be imputed, but here it s fr so we shouldn t remove extreme values, car elles\n",
    "# sont veridiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5ee765e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/13 00:49:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputPath = \"sf-stocks-clean.parquet\"\n",
    "\n",
    "noNullsDF = noNullsDF.withColumnRenamed(\"Adj Close\", \"AdjClose\")\n",
    "noNullsDF.write.mode(\"overwrite\").parquet(outputPath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}